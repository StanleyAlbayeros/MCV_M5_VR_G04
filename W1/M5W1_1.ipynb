{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5_Week01_AZNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "EemsuZtybd_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f419d414-4ff5-45d4-fed5-b5927045ae96"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPRim1xa7S-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4416c6a0-6f43-4f5b-abcc-c94a201d5a77"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BrIMwNdj-i7"
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "#import tensorflow as tf\n",
        "#device_name = tf.test.gpu_device_name()\n",
        "#if device_name != '/device:GPU:0':\n",
        "#  raise SystemError('GPU device not found')\n",
        "#print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3j30GALmsE"
      },
      "source": [
        "# **MyAZNet**  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFlVtblJQidu"
      },
      "source": [
        "### **Libraries.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SGuw17w45Ji"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import tarfile\n",
        "import torchvision\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets.utils import download_url"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LWN6lsub1-W"
      },
      "source": [
        "#Load libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnrVNzLW64o"
      },
      "source": [
        "#checking for device\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFzk9mspW8J3",
        "outputId": "01e5027c-5f29-4eb9-d91a-4801f3795a4d"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiFwz6Vl-JAu"
      },
      "source": [
        "#GPU\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUksYAqAQIyV"
      },
      "source": [
        "### **Parameters.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec72HIyjXWLY"
      },
      "source": [
        "#Transforms\n",
        "transformer=transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
        "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1F2fErqXf_w"
      },
      "source": [
        "#Dataloader\n",
        "\n",
        "#Path for training and testing directory\n",
        "train_path='../MIT_split/train'\n",
        "test_path='../MIT_split/test'\n",
        "writer = SummaryWriter('../summaryWriter/')\n",
        "train_writer = SummaryWriter('../summaryWriter/train')\n",
        "test_writer = SummaryWriter('../summaryWriter/test')\n",
        "\n",
        "train_loader=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
        "    batch_size=128, shuffle=True\n",
        "    )\n",
        "\n",
        "test_loader=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
        "    batch_size=128, shuffle=True\n",
        "    )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxmOzqjXz4t"
      },
      "source": [
        "#Image categories\n",
        "root=pathlib.Path(train_path)\n",
        "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCw0aOFzXz0T",
        "outputId": "11ffbfe0-4525-4e78-c3f5-1a0016c35342"
      },
      "source": [
        "print(classes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Opencountry', 'coast', 'forest', 'highway', 'inside_city', 'mountain', 'street', 'tallbuilding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdZHaCG4Xzrj"
      },
      "source": [
        "#CNN Network\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self,num_classes=8):\n",
        "        super(MLP,self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Conv1\n",
        "        self.conv1=torch.nn.Conv2d(3,64,3, padding=1)\n",
        "        #Shape= (128,3,224,224)\n",
        "        self.relu1=nn.ReLU()\n",
        "\n",
        "        #Batch Norm\n",
        "        self.bn1=torch.nn.BatchNorm2d(64)\n",
        "        #Shape= (128,12,224,224)\n",
        "        self.relu2=nn.ReLU()\n",
        "        #Shape= (128,12,224,224)\n",
        "\n",
        "        #MaxPool\n",
        "        self.pool1=torch.nn.MaxPool2d(2)\n",
        "        #Shape= (128,12,112,112)\n",
        "        \n",
        "        #Conv2\n",
        "        self.conv2=torch.nn.Conv2d(64,64,3)\n",
        "        #Shape= (32,112,112)\n",
        "        self.relu3=nn.ReLU()\n",
        "        #Shape= (32,112,112)\n",
        "\n",
        "        #Global Avg Pooling\n",
        "        self.pool2=torch.nn.AdaptiveAvgPool2d(2)\n",
        "\n",
        "        #Dense\n",
        "        self.fc=torch.nn.Linear(256*1*1,2048)\n",
        "        self.relu4=torch.nn.ReLU()\n",
        "\n",
        "        #Dropout\n",
        "        self.drop1 = torch.nn.Dropout(p=0.4)\n",
        "\n",
        "        #Dense\n",
        "        self.fc2=torch.nn.Linear(2048,8)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "       \n",
        "    #Feed forwad function\n",
        "        \n",
        "    def forward(self,input):\n",
        "\n",
        "        output=self.conv1(input)\n",
        "        output=self.relu1(output)\n",
        "\n",
        "        output=self.bn1(output)\n",
        "        output=self.relu2(output)\n",
        "\n",
        "        output=self.pool1(output)\n",
        "\n",
        "        output=self.conv2(output)\n",
        "        output=self.relu3(output)\n",
        "\n",
        "        output=self.pool2(output)\n",
        "        # print(output.shape)\n",
        "\n",
        "        output=output.view(output.size(0),-1)\n",
        "\n",
        "        output=self.fc(output)\n",
        "        output=self.relu4(output)\n",
        "\n",
        "        output=self.drop1(output)\n",
        "\n",
        "        output=output.view(output.size(0),-1)\n",
        "\n",
        "        output=self.fc2(output)            \n",
        "\n",
        "        output=self.softmax(output)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQFvRsP7XzeW"
      },
      "source": [
        "model=MLP(num_classes=8).to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10QYmYMQ4MeI",
        "outputId": "c1bb24ea-920b-40af-a088-0b66e3b20231"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary (model, input_size=(3,224,224))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n              ReLU-2         [-1, 64, 224, 224]               0\n       BatchNorm2d-3         [-1, 64, 224, 224]             128\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6         [-1, 64, 110, 110]          36,928\n              ReLU-7         [-1, 64, 110, 110]               0\n AdaptiveAvgPool2d-8             [-1, 64, 2, 2]               0\n            Linear-9                 [-1, 2048]         526,336\n             ReLU-10                 [-1, 2048]               0\n          Dropout-11                 [-1, 2048]               0\n           Linear-12                    [-1, 8]          16,392\n          Softmax-13                    [-1, 8]               0\n================================================================\nTotal params: 581,576\nTrainable params: 581,576\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 115.99\nParams size (MB): 2.22\nEstimated Total Size (MB): 118.78\n----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP72LKEpXzS1"
      },
      "source": [
        "#Optmizer and loss function\n",
        "optimizer=Adam(model.parameters(),lr=0.001)\n",
        "loss_function=nn.CrossEntropyLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zfijen9XzHb"
      },
      "source": [
        "num_epochs=100"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvq5K4TpXy6E"
      },
      "source": [
        "#calculating the size of training and testing images\n",
        "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
        "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbempU6FYjPU",
        "outputId": "d7e4373e-3f43-45e7-bc78-ccad37f61fde"
      },
      "source": [
        "print(train_count,test_count)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1881 807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsZmoN3yYjLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72da7f9d-33f5-476c-e8c8-049d2b720ac7"
      },
      "source": [
        "\n",
        "#Model training and saving best model\n",
        "#source: https://github.com/gaurav67890/Pytorch_Tutorials/blob/master/cnn-scratch-training.ipynb\n",
        "\n",
        "#Model training and saving best model\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "best_accuracy=0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    #Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy=0.0\n",
        "    train_loss=0.0\n",
        "    \n",
        "    for i, data in enumerate(train_loader,0):\n",
        "        if torch.cuda.is_available():\n",
        "            train_images, train_labels = data\n",
        "            train_images=Variable(train_images.cuda())\n",
        "            train_labels=Variable(train_labels.cuda())\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        train_outputs=model(train_images)\n",
        "        loss=loss_function(train_outputs,train_labels)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()*len(train_images)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # train_loss+= loss.cpu().data*train_images.size(0)\n",
        "        _,prediction=torch.max(train_outputs.data,1)\n",
        "        \n",
        "        train_accuracy+=int(torch.sum(prediction==train_labels.data))\n",
        "\n",
        "    features = train_images.view(-1, 128 * 128)\n",
        "    \n",
        "    train_accuracy=train_accuracy/train_count\n",
        "    train_loss=train_loss/train_count    \n",
        "\n",
        "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
        "  \n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "    \n",
        "    test_accuracy=0.0\n",
        "    val_loss=0.0\n",
        "    for i, (test_images,test_labels) in enumerate(test_loader):\n",
        "       if torch.cuda.is_available():\n",
        "           test_images=Variable(test_images.cuda())\n",
        "           test_labels=Variable(test_labels.cuda())\n",
        "            \n",
        "       test_outputs=model(test_images)\n",
        "       _,prediction=torch.max(test_outputs.data,1)\n",
        "       \n",
        "       temp_test_loss = loss_function(test_outputs, test_labels)\n",
        "       temp_test_loss.backward()\n",
        "       val_loss += temp_test_loss.item()*len(test_images)\n",
        "\n",
        "       test_accuracy+=int(torch.sum(prediction==test_labels.data))\n",
        "    \n",
        "    test_accuracy=test_accuracy/test_count\n",
        "    test_loss=val_loss/test_count\n",
        "        \n",
        "    print('Epoch: '+str(epoch+1)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
        "    \n",
        "    test_writer.add_scalar('Loss/test', test_loss, epoch+1)\n",
        "    test_writer.add_scalar('Acc/test', test_accuracy, epoch+1)\n",
        "    train_writer.add_scalar('Loss/train', train_loss, epoch+1)\n",
        "    train_writer.add_scalar('Acc/train', train_accuracy, epoch+1)\n",
        "    \n",
        "    # Save the best model\n",
        "    if test_accuracy>best_accuracy:\n",
        "       torch.save(model.state_dict(),'best_checkpoint.model')\n",
        "       print(\"new checkpoint\")\n",
        "       best_accuracy=test_accuracy\n",
        "writer.add_graph(model, train_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8yojcHMYjER"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP986GgXYi6t"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRYIB8IQZPLV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m9SZWFLZO94"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzTinpHLZO3R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhk56q4CZOwC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzwmkIPwZOkO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCwsTtoMYisP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nKlgebADKMs"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkL9z2drEx3Y"
      },
      "source": [
        "# **Hyperparametrization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPy9mn_PZgIC"
      },
      "source": [
        "# list all data in history\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.savefig('../logs/plots_full_data/accuracy_hyper.jpg')\n",
        "#plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWpJRP4dZji5"
      },
      "source": [
        "  # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.savefig('../logs/plots_full_data/loss_hyper.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}